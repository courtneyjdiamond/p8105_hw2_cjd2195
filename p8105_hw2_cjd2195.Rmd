---
title: "P8105 Homework 2"
author: "Courtney Diamond"
date: "09-26-2023"
output: github_document
---

## Problem 1

### First we're going to load our tidyverse library and first dataset, `pols_month.csv`. 

```{r}
library(tidyverse)
library(readr)

pols_month_df = 
  read_csv("data/pols-month.csv") |> 
  separate(mon, into = c("year", "month", "day"), sep = "-") |> 
  mutate(
    month = case_match(
      month,
      "01" ~ "January",
      "02" ~ "February",
      "03" ~ "March", 
      "04" ~ "April", 
      "05" ~ "May", 
      "06" ~ "June",
      "07" ~ "July",
      "08" ~ "August",
      "09" ~ "September", 
      "10" ~ "October", 
      "11" ~ "November", 
      "12" ~ "December",
    )
  ) |> 
  mutate(
    year = as.integer(year)
  ) |> 
  mutate(
    day = as.integer(day)
  ) |> 
  mutate(
    president =
      case_match(
        prez_gop,
        0 ~ "dem",
        1 ~ "rep"
      )
  ) |> 
  select(!prez_gop & !prez_dem & !day)

pols_month_df
```

The first dataset, `pols_month_df`, contains information relating to the political affiliations of elected federal leaders (Senate, House of Representatives, and President) and state governors for each month of each year since `r min(pull(pols_month_df, year))` up to June of `r max(pull(pols_month_df, year))`. The cleaned dataset has a size of `r nrow(pols_month_df)` rows (observations) and `r ncol(pols_month_df)` columns (variables), with key variables including the following: 

* `year` : the year of the observation
* `month` : the month of the observation
* `gov_gop` : the number of Republican governors
* `sen_gop` : the number of Republican Senators
* `rep_gop` : the number of Republican Representatives
* `gov_dem` : the number of Democratic governors
* `sen_dem` : the number of Democratic Senators
* `rep_dem` : the number of Democratic Representatives
* `president` : the affiliation of the US President (Republican or Democratic)


### Now we'll load the `snp` dataset.

First though, I can see that we'll need to transform years from two digits to four, so I'm also going to code in a vector that specifies which of the two-digit combinations belong to this century in order to help make the conversion a little easier. (I can do this because I know from reading the dataset description that the data covers from the 1950s to present day; if this were for 1900 up to present, I'd have to approach it differently.) 
```{r}
this_century_years = c(0:15)

snp_df = 
  read_csv("data/snp.csv") |> 
  separate(date, into = c("month", "day", "year"), sep = "/") |>
  mutate(
    month =
      case_match(
        month,
        "1" ~ "January",
        "2" ~ "February",
        "3" ~ "March",
        "4" ~ "April",
        "5" ~ "May",
        "6" ~ "June",
        "7" ~ "July",
        "8" ~ "August",
        "9" ~ "September",
        "10" ~ "October",
        "11" ~ "November",
        "12" ~ "December"
      )
  ) |> 
  mutate(
    year = as.integer(year)
  ) |> 
  mutate(
    year = if_else(year %in% this_century_years, year + 2000, year + 1900)
  ) |> 
  relocate(year, month) |> 
  select(!day)
  

snp_df
```

The second dataset, `snp_df`, contains information about the closing value of the Standard and Poor's stock index for each listed date. Originally, the dataset had discrete dates listed for each value; however, I have removed the day associated with the date, and now the values associated are only those of year and month. This allowed me to then merge this dataset with the one above, which did not have day-specific data. The cleaned dataset has a size of `r nrow(snp_df)` rows and `r ncol(snp_df)` columns, ranging in dates from January of `r min(pull(snp_df, year))` to July of `r max(pull(snp_df, year))`. Variables include: 

* `year` : the year of the observation
* `month` : the month of the observation
* `close` : the closing value of the S&P index

### Now we'll load the unemployment data. 

```{r}
unemp_df = 
  read_csv("data/unemployment.csv") |> 
  pivot_longer(
    Jan:Dec,
    names_to = "Month", 
    values_to = "unemp_pct"
  ) |> 
  mutate(
    Month = 
      case_match(
        Month, 
        "Jan" ~ "January",
        "Feb" ~ "February",
        "Mar" ~ "March",
        "Apr" ~ "April",
        "May" ~ "May",
        "Jun" ~ "June",
        "Jul" ~ "July",
        "Aug" ~ "August",
        "Sep" ~ "September",
        "Oct" ~ "October",
        "Nov" ~ "November",
        "Dec" ~ "December"
        )
  ) |> 
  rename(year = Year, month = Month)

unemp_df
```

The last dataset, `unemp_df`, contains the unemployment rate measured as a percentage for a given year and month. The cleaned dataset has a size of `r nrow(unemp_df)` rows and `r ncol(unemp_df)` columns, and contains information from January `r min(pull(unemp_df, year))` up to December of `r max(pull(unemp_df, year))` (though the last six months of December do not have percentages listed.) The variables include: 

* `year` : the year of the observation
* `month` : the month of the observation
* `unemp_pct` : the unemployment percentage in the US

### Now we'll merge the datasets.

Left-joining sequentially conesequently joins observations by year and month.

```{r}
fivethreeeight_df = left_join(
  pols_month_df, snp_df
) |> 
  left_join(unemp_df)

fivethreeeight_df
```

The resultant dataset is size 822x11, with the following variables. Any observations that are missing are denoted with `NA`. 

* `year` : the year of the observation
* `month` : the month of the observation
* `gov_gop` : the number of Republican governors
* `sen_gop` : the number of Republican Senators
* `rep_gop` : the number of Republican Representatives
* `gov_dem` : the number of Democratic governors
* `sen_dem` : the number of Democratic Senators
* `rep_dem` : the number of Democratic Representatives
* `president` : the affiliation of the US President (Republican or Democratic)
* `close` : the closing value of the S&P index
* `unemp_pct` : the unemployment percentage in the US



## Problem 2

### Let's load the Trash Wheel datasets. 

```{r}
library(readxl)

mr_trash_wheel_df = 
  read_excel("data/Trash Wheel Collection Data.xlsx", sheet = 1, range = "A2:N549") |> 
  janitor::clean_names() |> 
  mutate(
    houses_powered = (weight_tons * 500) / 30
  ) |> 
  select(!homes_powered) |> 
  mutate(
    trash_wheel = "Mr. Trash Wheel"
  ) |> 
  relocate(trash_wheel) |> 
  mutate(
    year = as.numeric(year)
  )

prof_trash_wheel_df = 
  read_excel("data/Trash Wheel Collection Data.xlsx", sheet = 2, range = "A2:M96") |> 
  janitor::clean_names() |> 
  mutate(
    houses_powered = (weight_tons * 500) / 30
  ) |> 
  select(!homes_powered) |> 
  mutate(
    trash_wheel = "Prof. Trash Wheel"
  ) |> 
  relocate(trash_wheel)

gwynnda_trash_wheel_df = 
  read_excel("data/Trash Wheel Collection Data.xlsx", sheet = 4, range = "A2:K108") |> 
  janitor::clean_names() |> 
  mutate(
    houses_powered = (weight_tons * 500) / 30
  ) |> 
  select(!homes_powered) |> 
  mutate(
    trash_wheel = "Gwynnda Trash Wheel"
  ) |> 
  relocate(trash_wheel)



trash_wheel_data_df = bind_rows(
  mr_trash_wheel_df,
  prof_trash_wheel_df,
  gwynnda_trash_wheel_df
) |> 
  relocate(plastic_bags, .after = sports_balls)

trash_wheel_data_df
```


Taking a moment to walk through all the above code: for each dataset, I first imported the respective sheet from the Excel document using the range I specified, created my own version of the `homes_powered` variable using the formula described in the original dataset, removed the old version of the `homes_powered` variable so as not to have duplicates with my own, then added a column to the dataset specifying which Trash Wheel the data belong to. Having done this, and assuring myself that columns represented across multiple datasets were in the same order and spelled the same, it was safe to use the `bind_rows` function to join the datasets together. In a final step, I moved the `plastic_bags` variable to come before the final `houses_powered` variable because I wanted to keep similar variables (i.e. counts of items found) grouped together. 

The resulting dataset `trash_wheel_data_df` is `r nrow(trash_wheel_data_df)` rows and `r ncol(trash_wheel_data_df)` columns in size, meaning there are `r nrow(trash_wheel_data_df)` observations and `r ncol(trash_wheel_data_df)` total variables. The variables include: 
 
* `trash_wheel` : which trash wheel the data come from 
* `dumpster` : a unique identifier of the dumpster the data come from
* `month` : the month of the observation
* `year` : the year of the observation
* `date` : the date of the observation
* `weight_tons` : the weight of all the trash collected, in tons
* `volume_cubic_yards` : the volume of all the trash collected, in cubic yards
* `plastic_bottles` : the number of plastic bottles collected
* `polystyrene` : the number of polystyrene items collected
* `cigarette_butts` : the number of cigarette butts collected
* `glass_bottles` : the number of glass bottles collected
* `grocery_bags` : the number of grocery bags collected
* `chip_bags` : the number of chip bags collected
* `sports_balls` : the number of sports balls collected
* `plastic_bags` : the number of plastic bags collected (it is unknown if these are the same as "grocery bags", so I have kept them as a separate variable)
* `houses_powered` : the number of homes able to be powered from the trash collected, based on the suggested calculation that each ton creates 500 kW and each home uses 30kW. 



The total amount of weight collected by Prof. Trash Wheel is `r sum(pull(filter(trash_wheel_data_df, trash_wheel == "Prof. Trash Wheel"), weight_tons))` tons.

The total number of cigarette butts collected by Gwynnda in July 2021 is `r format(sum(pull(filter(trash_wheel_data_df, trash_wheel == "Gwynnda Trash Wheel" & year == 2021 & month == "July"), cigarette_butts)), scientific = FALSE)`. 
 
## Problem 3

### Let's begin by importing the baseline data. 

```{r}
baseline_df  = 
  read_csv("data/MCI_baseline.csv", skip = 1) |> 
  janitor::clean_names() |> 
  select(!education) |> 
  mutate(
    sex = 
      case_match(
        sex,
        0 ~ "Female",
        1 ~ "Male"
      )
  ) |> 
  mutate(
    apoe4 = 
      case_match(
        apoe4,
        0 ~ "Non-carrier",
        1 ~ "Carrier"
      )
  ) |> 
  mutate(
    age_at_onset = as.numeric(age_at_onset)
  ) |> 
  rename(
    "age_at_baseline" = "current_age"
  ) |> 
  rename(
    "age_at_mci_onset" = "age_at_onset"
  ) |> 
  mutate(
    time_to_onset = 
      age_at_mci_onset - age_at_baseline 
  ) |> 
  filter(is.na(time_to_onset) | time_to_onset > 0) |> 
  relocate(id, sex, apoe4)
  

baseline_df
```

Going step by step to explain the process: first I imported the data; a quick view of the CSV file showed me there is an additional row of notes at the top of the file that I didn't want to import, so I skipped that row when importing. We're not interested in the education demographic data, so I dropped that column. Then I wanted to make my data a little more human-readable, so I converted the 0s and 1s in the sex and APOE4 columns to more sensical labels. Next, I wanted to consider whether or not the patients in this file were actually eligible for the study; I can figure out whether or not patients had MCI at baseline by subtracting their baseline age from the age at which they developed MCI. However, to do this, I needed to make sure both columns were numeric values, so I converted the "age at onset" variable because it was originally character values. I also renamed these variables for a little more clarity; then I created the "time_to_onset" variable as described. I finally filtered on this variable to determine my eligible patient population: if the value in this column is equal to or less than 0, the patient had previously developed MCI or had it at baseline, making them ineligible. However, I also needed to keep in any patients who have NA values in this column; this is derived from the "age at onset" column, indicating that they have never developed MCI. If I just filtered to search for those with a "time to onset" of > 0, all patients who had never developed MCI would have been dropped; thus, I included them in my filter explicitly. Lastly, I rearranged the columns to keep all the age-related variables together to make it a little easier to read and quickly understand.

A quick summary of the variables: 

* `id` : study ID
* `sex` : participant sex
* `apoe4` : carrier status
* `age_at_baseline` : age at baseline measurement
* `age_at_mci_onset` : age at mci onset (NA if no onset)
* `time_to_onset` : time (years) between baseline measurement and mci onset (NA if no onset)

Ultimately, a total of `r n_distinct(pull(baseline_df, id))` patients were recruited for (i.e. were eligible for) the study. (If we aren't satisfied by selecting the distinct patient IDs to determine this, we can also use the quick-and-dirty count of all the rows in the dataframe, `r nrow(baseline_df)`.)

A total of `r sum(!is.na(pull(baseline_df, time_to_onset)))` patients went on to develop MCI.

The average age at the baseline measurement was `r mean(pull(baseline_df, age_at_baseline))` years.

There were `r nrow(filter(baseline_df, sex == "Female"))` female participants, of which `r nrow(filter(baseline_df, sex == "Female" & apoe4 == "Carrier"))` were carriers of APOE4, a proportion of `r (nrow(filter(baseline_df, sex == "Female" & apoe4 == "Carrier"))) / (nrow(filter(baseline_df, sex == "Female")))`. 



### Let's now look at the longitudinal data. 

```{r}
amyloid_ratio_df = 
  read_csv("data/mci_amyloid.csv", skip = 1) |> 
  janitor::clean_names() |> 
  rename(
    "id" = "study_id"
  ) |> 
  rename(
    "time_0" = "baseline"
  ) |> 
  mutate(
    time_0 = as.numeric(time_0),
    time_2 = as.numeric(time_2),
    time_4 = as.numeric(time_4),
    time_6 = as.numeric(time_6),
    time_8 = as.numeric(time_8) 
  )

amyloid_ratio_df
```


With respect to the amyloid ratio data import process: once again, there was an extra row of information at the top that was not pertinent to the data itself, so I skipped it. Anticipating needing to join these data to the baseline data, I renamed the "Study ID" variable to be consistent with the "id" nomenclature I used in the baseline data. Similarly, because I myself got confused when reading through the original column names trying to understand whether the native "baseline" column was a time measurement or an amyloid ratio measurement, I renamed the column to "time_0" to be consistent with the other columns once I had figured out that they were all amyloid ratio measurements taken across different time points. Finally, anticipating downstream data needs for this project, I converted all the measurement columns to a numeric form rather than character. 

A summary of the variables: 

* `id` : study id
* `time_x`: the amyloid ratio measurement taken at X years after the baseline observation (X = 0 denoting the baseline measure)

There are a total of `r nrow(amyloid_ratio_df)` participants in the longitudinal dataset. 

### Let's see which participants are in both tables, or only in one table. 

```{r}
anti_join(baseline_df, amyloid_ratio_df, by = "id")

anti_join(amyloid_ratio_df, baseline_df, by = "id")
```

We need to run both anti-joins because we want to know who is absent from each table, and a single anti-join only tells us who (from the table listed first) is not in the second table. 

We can see that there are `r nrow(anti_join(baseline_df, amyloid_ratio_df, by = "id"))` participants in the baseline table who do not have data in the longitudinal data set; these are IDs 14, 49, 92, 179, 268, 304, 389, and 412. These folks may have withdrawn from the study before any of the measurements were taken, but I can't say for sure why they're not there. 

There are `r nrow(anti_join(amyloid_ratio_df, baseline_df, by = "id"))` participants in the longitudinal dataset who are not in the baseline data (likely because we've cleaned out folks who ended up not being eligible for the study). These are IDs 72, 234, 283, 380, 484-495. The long list of sequential IDs is a little alarming- it indicates there might be a systematic flaw with how the data were cleaned. However, I did double check my raw baseline datafile, and in fact none of these folks are in the original file, so there is likely some study-procedure-related reason for their exclusion from the baseline data measurements. 

### Let's combine the tables! 
Combine the demographic and biomarker datasets so that only participants who appear in both datasets are retained, and briefly describe the resulting dataset; export the result as a CSV to your data directory.]

To create the final dataset that contains only folks who are in both the baseline and longitudinal datasets, I'm going to use an inner join on the study ID. 

```{r}
full_alz_data_df = 
  inner_join(baseline_df, amyloid_ratio_df, by = "id")

```

To double check that my join worked, I can subtract the number of folks who were in my baseline dataset but not longitudinal (`r nrow(anti_join(baseline_df, amyloid_ratio_df, by = "id"))` people) from the number of folks in my baseline dataset (`r n_distinct(pull(baseline_df, id))` people) and should ideally get the same number as the length of my new full dataset (`r nrow(full_alz_data_df)` people). We can do this for the other table too: `r nrow(amyloid_ratio_df)` -  `r nrow(anti_join(amyloid_ratio_df, baseline_df, by = "id"))` = `r nrow(full_alz_data_df)` people. The math maths, so let's export it. 

```{r}
write_csv(full_alz_data_df, "full_alz_data_df.csv")
```

