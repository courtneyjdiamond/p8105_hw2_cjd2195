P8105 Homework 2
================
Courtney Diamond
09-26-2023

## Problem 1

### First we’re going to load our tidyverse library and first dataset, `pols_month.csv`.

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.3     ✔ readr     2.1.4
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.0
    ## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readr)

pols_month_df = 
  read_csv("data/pols-month.csv") |> 
  separate(mon, into = c("year", "month", "day"), sep = "-") |> 
  mutate(
    month = case_match(
      month,
      "01" ~ "January",
      "02" ~ "February",
      "03" ~ "March", 
      "04" ~ "April", 
      "05" ~ "May", 
      "06" ~ "June",
      "07" ~ "July",
      "08" ~ "August",
      "09" ~ "September", 
      "10" ~ "October", 
      "11" ~ "November", 
      "12" ~ "December",
    )
  ) |> 
  mutate(
    year = as.integer(year)
  ) |> 
  mutate(
    day = as.integer(day)
  ) |> 
  mutate(
    president =
      case_match(
        prez_gop,
        0 ~ "dem",
        1 ~ "rep"
      )
  ) |> 
  select(!prez_gop & !prez_dem & !day)
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
pols_month_df
```

    ## # A tibble: 822 × 9
    ##     year month     gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president
    ##    <int> <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>    
    ##  1  1947 January        23      51     253      23      45     198 dem      
    ##  2  1947 February       23      51     253      23      45     198 dem      
    ##  3  1947 March          23      51     253      23      45     198 dem      
    ##  4  1947 April          23      51     253      23      45     198 dem      
    ##  5  1947 May            23      51     253      23      45     198 dem      
    ##  6  1947 June           23      51     253      23      45     198 dem      
    ##  7  1947 July           23      51     253      23      45     198 dem      
    ##  8  1947 August         23      51     253      23      45     198 dem      
    ##  9  1947 September      23      51     253      23      45     198 dem      
    ## 10  1947 October        23      51     253      23      45     198 dem      
    ## # ℹ 812 more rows

The first dataset, `pols_month_df`, contains information relating to the
political affiliations of elected federal leaders (Senate, House of
Representatives, and President) and state governors for each month of
each year since 1947 up to June of 2015. The cleaned dataset has a size
of 822 rows (observations) and 9 columns (variables), with key variables
including the following:

- `year` : the year of the observation
- `month` : the month of the observation
- `gov_gop` : the number of Republican governors
- `sen_gop` : the number of Republican Senators
- `rep_gop` : the number of Republican Representatives
- `gov_dem` : the number of Democratic governors
- `sen_dem` : the number of Democratic Senators
- `rep_dem` : the number of Democratic Representatives
- `president` : the affiliation of the US President (Republican or
  Democratic)

### Now we’ll load the `snp` dataset.

First though, I can see that we’ll need to transform years from two
digits to four, so I’m also going to code in a vector that specifies
which of the two-digit combinations belong to this century in order to
help make the conversion a little easier. (I can do this because I know
from reading the dataset description that the data covers from the 1950s
to present day; if this were for 1900 up to present, I’d have to
approach it differently.)

``` r
this_century_years = c(0:15)

snp_df = 
  read_csv("data/snp.csv") |> 
  separate(date, into = c("month", "day", "year"), sep = "/") |>
  mutate(
    month =
      case_match(
        month,
        "1" ~ "January",
        "2" ~ "February",
        "3" ~ "March",
        "4" ~ "April",
        "5" ~ "May",
        "6" ~ "June",
        "7" ~ "July",
        "8" ~ "August",
        "9" ~ "September",
        "10" ~ "October",
        "11" ~ "November",
        "12" ~ "December"
      )
  ) |> 
  mutate(
    year = as.integer(year)
  ) |> 
  mutate(
    year = if_else(year %in% this_century_years, year + 2000, year + 1900)
  ) |> 
  relocate(year, month) |> 
  select(!day)
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
snp_df
```

    ## # A tibble: 787 × 3
    ##     year month    close
    ##    <dbl> <chr>    <dbl>
    ##  1  2015 July     2080.
    ##  2  2015 June     2063.
    ##  3  2015 May      2107.
    ##  4  2015 April    2086.
    ##  5  2015 March    2068.
    ##  6  2015 February 2104.
    ##  7  2015 January  1995.
    ##  8  2014 December 2059.
    ##  9  2014 November 2068.
    ## 10  2014 October  2018.
    ## # ℹ 777 more rows

The second dataset, `snp_df`, contains information about the closing
value of the Standard and Poor’s stock index for each listed date.
Originally, the dataset had discrete dates listed for each value;
however, I have removed the day associated with the date, and now the
values associated are only those of year and month. This allowed me to
then merge this dataset with the one above, which did not have
day-specific data. The cleaned dataset has a size of 787 rows and 3
columns, ranging in dates from January of 1950 to July of 2015.
Variables include:

- `year` : the year of the observation
- `month` : the month of the observation
- `close` : the closing value of the S&P index

### Now we’ll load the unemployment data.

``` r
unemp_df = 
  read_csv("data/unemployment.csv") |> 
  pivot_longer(
    Jan:Dec,
    names_to = "Month", 
    values_to = "unemp_pct"
  ) |> 
  mutate(
    Month = 
      case_match(
        Month, 
        "Jan" ~ "January",
        "Feb" ~ "February",
        "Mar" ~ "March",
        "Apr" ~ "April",
        "May" ~ "May",
        "Jun" ~ "June",
        "Jul" ~ "July",
        "Aug" ~ "August",
        "Sep" ~ "September",
        "Oct" ~ "October",
        "Nov" ~ "November",
        "Dec" ~ "December"
        )
  ) |> 
  rename(year = Year, month = Month)
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
unemp_df
```

    ## # A tibble: 816 × 3
    ##     year month     unemp_pct
    ##    <dbl> <chr>         <dbl>
    ##  1  1948 January         3.4
    ##  2  1948 February        3.8
    ##  3  1948 March           4  
    ##  4  1948 April           3.9
    ##  5  1948 May             3.5
    ##  6  1948 June            3.6
    ##  7  1948 July            3.6
    ##  8  1948 August          3.9
    ##  9  1948 September       3.8
    ## 10  1948 October         3.7
    ## # ℹ 806 more rows

The last dataset, `unemp_df`, contains the unemployment rate measured as
a percentage for a given year and month. The cleaned dataset has a size
of 816 rows and 3 columns, and contains information from January 1948 up
to December of 2015 (though the last six months of December do not have
percentages listed.) The variables include:

- `year` : the year of the observation
- `month` : the month of the observation
- `unemp_pct` : the unemployment percentage in the US

### Now we’ll merge the datasets.

Left-joining sequentially conesequently joins observations by year and
month.

``` r
fivethreeeight_df = left_join(
  pols_month_df, snp_df
) |> 
  left_join(unemp_df)
```

    ## Joining with `by = join_by(year, month)`
    ## Joining with `by = join_by(year, month)`

``` r
fivethreeeight_df
```

    ## # A tibble: 822 × 11
    ##     year month   gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president close
    ##    <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>     <dbl>
    ##  1  1947 January      23      51     253      23      45     198 dem          NA
    ##  2  1947 Februa…      23      51     253      23      45     198 dem          NA
    ##  3  1947 March        23      51     253      23      45     198 dem          NA
    ##  4  1947 April        23      51     253      23      45     198 dem          NA
    ##  5  1947 May          23      51     253      23      45     198 dem          NA
    ##  6  1947 June         23      51     253      23      45     198 dem          NA
    ##  7  1947 July         23      51     253      23      45     198 dem          NA
    ##  8  1947 August       23      51     253      23      45     198 dem          NA
    ##  9  1947 Septem…      23      51     253      23      45     198 dem          NA
    ## 10  1947 October      23      51     253      23      45     198 dem          NA
    ## # ℹ 812 more rows
    ## # ℹ 1 more variable: unemp_pct <dbl>

The resultant dataset is size 822x11, with the following variables. Any
observations that are missing are denoted with `NA`.

- `year` : the year of the observation
- `month` : the month of the observation
- `gov_gop` : the number of Republican governors
- `sen_gop` : the number of Republican Senators
- `rep_gop` : the number of Republican Representatives
- `gov_dem` : the number of Democratic governors
- `sen_dem` : the number of Democratic Senators
- `rep_dem` : the number of Democratic Representatives
- `president` : the affiliation of the US President (Republican or
  Democratic)
- `close` : the closing value of the S&P index
- `unemp_pct` : the unemployment percentage in the US

## Problem 2

### Let’s load the Trash Wheel datasets.

``` r
library(readxl)

mr_trash_wheel_df = 
  read_excel("data/Trash Wheel Collection Data.xlsx", sheet = 1, range = "A2:N549") |> 
  janitor::clean_names() |> 
  mutate(
    houses_powered = (weight_tons * 500) / 30
  ) |> 
  select(!homes_powered) |> 
  mutate(
    trash_wheel = "Mr. Trash Wheel"
  ) |> 
  relocate(trash_wheel) |> 
  mutate(
    year = as.numeric(year)
  )

prof_trash_wheel_df = 
  read_excel("data/Trash Wheel Collection Data.xlsx", sheet = 2, range = "A2:M96") |> 
  janitor::clean_names() |> 
  mutate(
    houses_powered = (weight_tons * 500) / 30
  ) |> 
  select(!homes_powered) |> 
  mutate(
    trash_wheel = "Prof. Trash Wheel"
  ) |> 
  relocate(trash_wheel)

gwynnda_trash_wheel_df = 
  read_excel("data/Trash Wheel Collection Data.xlsx", sheet = 4, range = "A2:K108") |> 
  janitor::clean_names() |> 
  mutate(
    houses_powered = (weight_tons * 500) / 30
  ) |> 
  select(!homes_powered) |> 
  mutate(
    trash_wheel = "Gwynnda Trash Wheel"
  ) |> 
  relocate(trash_wheel)



trash_wheel_data_df = bind_rows(
  mr_trash_wheel_df,
  prof_trash_wheel_df,
  gwynnda_trash_wheel_df
) |> 
  relocate(plastic_bags, .after = sports_balls)

trash_wheel_data_df
```

    ## # A tibble: 747 × 16
    ##    trash_wheel     dumpster month  year date                weight_tons
    ##    <chr>              <dbl> <chr> <dbl> <dttm>                    <dbl>
    ##  1 Mr. Trash Wheel        1 May    2014 2014-05-16 00:00:00        4.31
    ##  2 Mr. Trash Wheel        2 May    2014 2014-05-16 00:00:00        2.74
    ##  3 Mr. Trash Wheel        3 May    2014 2014-05-16 00:00:00        3.45
    ##  4 Mr. Trash Wheel        4 May    2014 2014-05-17 00:00:00        3.1 
    ##  5 Mr. Trash Wheel        5 May    2014 2014-05-17 00:00:00        4.06
    ##  6 Mr. Trash Wheel        6 May    2014 2014-05-20 00:00:00        2.71
    ##  7 Mr. Trash Wheel        7 May    2014 2014-05-21 00:00:00        1.91
    ##  8 Mr. Trash Wheel        8 May    2014 2014-05-28 00:00:00        3.7 
    ##  9 Mr. Trash Wheel        9 June   2014 2014-06-05 00:00:00        2.52
    ## 10 Mr. Trash Wheel       10 June   2014 2014-06-11 00:00:00        3.76
    ## # ℹ 737 more rows
    ## # ℹ 10 more variables: volume_cubic_yards <dbl>, plastic_bottles <dbl>,
    ## #   polystyrene <dbl>, cigarette_butts <dbl>, glass_bottles <dbl>,
    ## #   grocery_bags <dbl>, chip_bags <dbl>, sports_balls <dbl>,
    ## #   plastic_bags <dbl>, houses_powered <dbl>

Taking a moment to walk through all the above code: for each dataset, I
first imported the respective sheet from the Excel document using the
range I specified, created my own version of the `homes_powered`
variable using the formula described in the original dataset, removed
the old version of the `homes_powered` variable so as not to have
duplicates with my own, then added a column to the dataset specifying
which Trash Wheel the data belong to. Having done this, and assuring
myself that columns represented across multiple datasets were in the
same order and spelled the same, it was safe to use the `bind_rows`
function to join the datasets together. In a final step, I moved the
`plastic_bags` variable to come before the final `houses_powered`
variable because I wanted to keep similar variables (i.e. counts of
items found) grouped together.

The resulting dataset `trash_wheel_data_df` is 747 rows and 16 columns
in size, meaning there are 747 observations and 16 total variables. The
variables include:

- `trash_wheel` : which trash wheel the data come from
- `dumpster` : a unique identifier of the dumpster the data come from
- `month` : the month of the observation
- `year` : the year of the observation
- `date` : the date of the observation
- `weight_tons` : the weight of all the trash collected, in tons
- `volume_cubic_yards` : the volume of all the trash collected, in cubic
  yards
- `plastic_bottles` : the number of plastic bottles collected
- `polystyrene` : the number of polystyrene items collected
- `cigarette_butts` : the number of cigarette butts collected
- `glass_bottles` : the number of glass bottles collected
- `grocery_bags` : the number of grocery bags collected
- `chip_bags` : the number of chip bags collected
- `sports_balls` : the number of sports balls collected
- `plastic_bags` : the number of plastic bags collected (it is unknown
  if these are the same as “grocery bags”, so I have kept them as a
  separate variable)
- `houses_powered` : the number of homes able to be powered from the
  trash collected, based on the suggested calculation that each ton
  creates 500 kW and each home uses 30kW.

The total amount of weight collected by Prof. Trash Wheel is 190.12
tons.

The total number of cigarette butts collected by Gwynnda in July 2021 is
16300.

## Problem 3

### Let’s begin by importing the baseline data.

``` r
baseline_df  = 
  read_csv("data/MCI_baseline.csv", skip = 1) |> 
  janitor::clean_names() |> 
  select(!education) |> 
  mutate(
    sex = 
      case_match(
        sex,
        0 ~ "Female",
        1 ~ "Male"
      )
  ) |> 
  mutate(
    apoe4 = 
      case_match(
        apoe4,
        0 ~ "Non-carrier",
        1 ~ "Carrier"
      )
  ) |> 
  mutate(
    age_at_onset = as.numeric(age_at_onset)
  ) |> 
  rename(
    "age_at_baseline" = "current_age"
  ) |> 
  rename(
    "age_at_mci_onset" = "age_at_onset"
  ) |> 
  mutate(
    time_to_onset = 
      age_at_mci_onset - age_at_baseline 
  ) |> 
  filter(is.na(time_to_onset) | time_to_onset > 0) |> 
  relocate(id, sex, apoe4)
```

    ## Rows: 483 Columns: 6
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): Age at onset
    ## dbl (5): ID, Current Age, Sex, Education, apoe4
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `age_at_onset = as.numeric(age_at_onset)`.
    ## Caused by warning:
    ## ! NAs introduced by coercion

``` r
baseline_df
```

    ## # A tibble: 479 × 6
    ##       id sex    apoe4       age_at_baseline age_at_mci_onset time_to_onset
    ##    <dbl> <chr>  <chr>                 <dbl>            <dbl>         <dbl>
    ##  1     1 Female Carrier                63.1             NA           NA   
    ##  2     2 Female Carrier                65.6             NA           NA   
    ##  3     3 Male   Carrier                62.5             66.8          4.30
    ##  4     4 Female Non-carrier            69.8             NA           NA   
    ##  5     5 Male   Non-carrier            66               68.7          2.70
    ##  6     6 Male   Non-carrier            62.5             NA           NA   
    ##  7     7 Male   Non-carrier            66.5             74            7.5 
    ##  8     8 Female Non-carrier            67.2             NA           NA   
    ##  9     9 Female Non-carrier            66.7             NA           NA   
    ## 10    10 Female Non-carrier            64.1             NA           NA   
    ## # ℹ 469 more rows

Going step by step to explain the process: first I imported the data; a
quick view of the CSV file showed me there is an additional row of notes
at the top of the file that I didn’t want to import, so I skipped that
row when importing. We’re not interested in the education demographic
data, so I dropped that column. Then I wanted to make my data a little
more human-readable, so I converted the 0s and 1s in the sex and APOE4
columns to more sensical labels. Next, I wanted to consider whether or
not the patients in this file were actually eligible for the study; I
can figure out whether or not patients had MCI at baseline by
subtracting their baseline age from the age at which they developed MCI.
However, to do this, I needed to make sure both columns were numeric
values, so I converted the “age at onset” variable because it was
originally character values. I also renamed these variables for a little
more clarity; then I created the “time_to_onset” variable as described.
I finally filtered on this variable to determine my eligible patient
population: if the value in this column is equal to or less than 0, the
patient had previously developed MCI or had it at baseline, making them
ineligible. However, I also needed to keep in any patients who have NA
values in this column; this is derived from the “age at onset” column,
indicating that they have never developed MCI. If I just filtered to
search for those with a “time to onset” of \> 0, all patients who had
never developed MCI would have been dropped; thus, I included them in my
filter explicitly. Lastly, I rearranged the columns to keep all the
age-related variables together to make it a little easier to read and
quickly understand.

A quick summary of the variables:

- `id` : study ID
- `sex` : participant sex
- `apoe4` : carrier status
- `age_at_baseline` : age at baseline measurement
- `age_at_mci_onset` : age at mci onset (NA if no onset)
- `time_to_onset` : time (years) between baseline measurement and mci
  onset (NA if no onset)

Ultimately, a total of 479 patients were recruited for (i.e. were
eligible for) the study. (If we aren’t satisfied by selecting the
distinct patient IDs to determine this, we can also use the
quick-and-dirty count of all the rows in the dataframe, 479.)

A total of 93 patients went on to develop MCI.

The average age at the baseline measurement was 65.0286013 years.

There were 210 female participants, of which 63 were carriers of APOE4,
a proportion of 0.3.

### Let’s now look at the longitudinal data.

``` r
amyloid_ratio_df = 
  read_csv("data/mci_amyloid.csv", skip = 1) |> 
  janitor::clean_names() |> 
  rename(
    "id" = "study_id"
  ) |> 
  rename(
    "time_0" = "baseline"
  ) |> 
  mutate(
    time_0 = as.numeric(time_0),
    time_2 = as.numeric(time_2),
    time_4 = as.numeric(time_4),
    time_6 = as.numeric(time_6),
    time_8 = as.numeric(time_8) 
  )
```

    ## Rows: 487 Columns: 6
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (5): Baseline, Time 2, Time 4, Time 6, Time 8
    ## dbl (1): Study ID
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

    ## Warning: There were 5 warnings in `mutate()`.
    ## The first warning was:
    ## ℹ In argument: `time_0 = as.numeric(time_0)`.
    ## Caused by warning:
    ## ! NAs introduced by coercion
    ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.

``` r
amyloid_ratio_df
```

    ## # A tibble: 487 × 6
    ##       id time_0 time_2 time_4 time_6 time_8
    ##    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>
    ##  1     1  0.111 NA      0.109  0.105  0.107
    ##  2     2  0.107  0.109  0.109  0.106  0.107
    ##  3     3  0.106  0.109  0.106 NA      0.106
    ##  4     4  0.109  0.109  0.111  0.107  0.111
    ##  5     5  0.108  0.112  0.115  0.107  0.106
    ##  6     6  0.112  0.113  0.111  0.110  0.115
    ##  7     7  0.112 NA      0.104  0.112  0.112
    ##  8     8  0.110  0.109 NA      0.109  0.110
    ##  9     9  0.112  0.110  0.109 NA     NA    
    ## 10    10  0.112  0.112 NA     NA      0.110
    ## # ℹ 477 more rows

With respect to the amyloid ratio data import process: once again, there
was an extra row of information at the top that was not pertinent to the
data itself, so I skipped it. Anticipating needing to join these data to
the baseline data, I renamed the “Study ID” variable to be consistent
with the “id” nomenclature I used in the baseline data. Similarly,
because I myself got confused when reading through the original column
names trying to understand whether the native “baseline” column was a
time measurement or an amyloid ratio measurement, I renamed the column
to “time_0” to be consistent with the other columns once I had figured
out that they were all amyloid ratio measurements taken across different
time points. Finally, anticipating downstream data needs for this
project, I converted all the measurement columns to a numeric form
rather than character.

A summary of the variables:

- `id` : study id
- `time_x`: the amyloid ratio measurement taken at X years after the
  baseline observation (X = 0 denoting the baseline measure)

There are a total of 487 participants in the longitudinal dataset.

### Let’s see which participants are in both tables, or only in one table.

``` r
anti_join(baseline_df, amyloid_ratio_df, by = "id")
```

    ## # A tibble: 8 × 6
    ##      id sex    apoe4       age_at_baseline age_at_mci_onset time_to_onset
    ##   <dbl> <chr>  <chr>                 <dbl>            <dbl>         <dbl>
    ## 1    14 Female Non-carrier            58.4             66.2          7.80
    ## 2    49 Male   Non-carrier            64.7             68.4          3.70
    ## 3    92 Female Non-carrier            68.6             NA           NA   
    ## 4   179 Male   Non-carrier            68.1             NA           NA   
    ## 5   268 Female Carrier                61.4             67.5          6.1 
    ## 6   304 Female Non-carrier            63.8             NA           NA   
    ## 7   389 Female Non-carrier            59.3             NA           NA   
    ## 8   412 Male   Carrier                67               NA           NA

``` r
anti_join(amyloid_ratio_df, baseline_df, by = "id")
```

    ## # A tibble: 16 × 6
    ##       id time_0 time_2 time_4 time_6 time_8
    ##    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>
    ##  1    72  0.107 NA      0.107  0.107 NA    
    ##  2   234  0.111  0.111  0.110  0.107  0.109
    ##  3   283  0.113  0.107  0.113  0.108  0.114
    ##  4   380  0.111  0.105  0.107  0.105  0.110
    ##  5   484  0.111  0.111  0.109  0.111  0.107
    ##  6   485  0.106  0.105  0.108  0.107  0.106
    ##  7   486  0.109  0.115 NA      0.110  0.107
    ##  8   487  0.111  0.108  0.110  0.111  0.106
    ##  9   488  0.110  0.112  0.113  0.109  0.109
    ## 10   489  0.115  0.113  0.115  0.116  0.112
    ## 11   490  0.112  0.110  0.111  0.110 NA    
    ## 12   491  0.117  0.114  0.111  0.111  0.111
    ## 13   492  0.110  0.110  0.111  0.109  0.109
    ## 14   493  0.108  0.108  0.109  0.104  0.109
    ## 15   494  0.117  0.110  0.112  0.111  0.109
    ## 16   495 NA      0.105  0.108  0.106  0.103

We need to run both anti-joins because we want to know who is absent
from each table, and a single anti-join only tells us who (from the
table listed first) is not in the second table.

We can see that there are 8 participants in the baseline table who do
not have data in the longitudinal data set; these are IDs 14, 49, 92,
179, 268, 304, 389, and 412. These folks may have withdrawn from the
study before any of the measurements were taken, but I can’t say for
sure why they’re not there.

There are 16 participants in the longitudinal dataset who are not in the
baseline data (likely because we’ve cleaned out folks who ended up not
being eligible for the study). These are IDs 72, 234, 283, 380, 484-495.
The long list of sequential IDs is a little alarming- it indicates there
might be a systematic flaw with how the data were cleaned. However, I
did double check my raw baseline datafile, and in fact none of these
folks are in the original file, so there is likely some
study-procedure-related reason for their exclusion from the baseline
data measurements.

### Let’s combine the tables!

Combine the demographic and biomarker datasets so that only participants
who appear in both datasets are retained, and briefly describe the
resulting dataset; export the result as a CSV to your data directory.\]

To create the final dataset that contains only folks who are in both the
baseline and longitudinal datasets, I’m going to use an inner join on
the study ID.

``` r
full_alz_data_df = 
  inner_join(baseline_df, amyloid_ratio_df, by = "id")
```

To double check that my join worked, I can subtract the number of folks
who were in my baseline dataset but not longitudinal (8 people) from the
number of folks in my baseline dataset (479 people) and should ideally
get the same number as the length of my new full dataset (471 people).
We can do this for the other table too: 487 - 16 = 471 people. The math
maths, so let’s export it.

``` r
write_csv(full_alz_data_df, "full_alz_data_df.csv")
```
